{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** [**Project Plan**](#Project_Plan)<br>\n",
    "**2.** [**Data Acquistion**](#acquire_data)<br>\n",
    "**3.** [**Exploration**](#explore_data)<br>\n",
    "**4.** [**Clustering**](#cluster_data)<br>\n",
    "**5.** [**Modeling**](#model_data)<br>\n",
    "**6.** [**Conclusions**](#conclusions)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "> Using 2017 properties and prediction data from our Zillow database for single unit/single family homes, we were tasking with improving the log error (Zestimate)\n",
    "> To accomplish this the team used clustering methodologies to find patterns in terms of which features had the greatest effect on log error\n",
    "> Using the features which produced the best clusters, create a model to predict logerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    "**The deliverables for this project are the following data assets:**\n",
    "- Report detailing our analysis in an .ipynb format\n",
    "- Detailed README on a Github and repo containing all files for this project\n",
    "- All .py files that are necessary to reproducible work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: -c: line 0: syntax error near unexpected token `clustering.png'\r\n",
      "/bin/sh: -c: line 0: `[clustering](clustering.png)'\r\n"
     ]
    }
   ],
   "source": [
    "![clustering](clustering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<button class=\"button-save large\">[Clustering Zillow Logerror](https://chasethompson.github.io)</button>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Project_Plan'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# explore/ stat\n",
    "import scipy.stats as stats\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# support modules\n",
    "import acquire\n",
    "import wrangle\n",
    "import prepare\n",
    "import split_scale\n",
    "import cluster\n",
    "import model\n",
    "\n",
    "# look at function for summarize plot\n",
    "# import summarize\n",
    "# import prepare\n",
    "# import explore\n",
    "# import split_scale\n",
    "\n",
    "# modeling\n",
    "from sklearn.cluster import KMeans, dbscan\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "# view full DF\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# some handy functions to use along widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "# widget packages\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# our conclusion button\n",
    "button = widgets.Button(description='Our Conclusion')\n",
    "out = widgets.Output()\n",
    "def on_button_clicked(_):\n",
    "      # \"linking function with output\"\n",
    "      with out:\n",
    "          # what happens when we press the button\n",
    "          clear_output()\n",
    "          print('Hypothesis and conclusion is unclear.')\n",
    "          print('Our derived variables proved useful, but not significantly.')\n",
    "          print('The \"best\" performing model was a decision tree.')\n",
    "# linking button and function together using a button's method\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "conclusion = widgets.VBox([button,out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='acquire_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aquisition and Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve from the Codeup Zillow_db:**\n",
    "\n",
    "- Latest transactions occured within the year of 2017 for each property\n",
    "- Logerror\n",
    "- All fields related to each properties\n",
    "    - Fields known to have greater than 60% nulls were excluded from the query\n",
    "- Gather descriptions by joining description tables\n",
    "- Only properties where latitude and longitude are not null\n",
    "- Only properties where bedroom and bathroom count were not 0\n",
    "- Only single family homes (SFR)\n",
    "\n",
    "To create this dataframe use the get_zillow_data function from the acquire.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_zillow_data()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following transofmations we achieved through a series of functions:\n",
    "> - ensure there are no nulls\n",
    "> - remove all outliers are removed\n",
    "> - create dervied features \n",
    "> - assigned intuitive names to each feature\n",
    "\n",
    "- All datatypes are appropriate for our use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle.handle_nulls(df)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create synthetic features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle.prepare_zillow(df)\n",
    "\n",
    "df = prepare.tax_rate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers in these coloumns\n",
    "col_out = [\"bathroomcnt\", \"bedroomcnt\", \"tax_rate\", \"calculatedfinishedsquarefeet\", \n",
    "           \"lotsizesquarefeet\", \"structuretaxvaluedollarcnt\", \"taxvaluedollarcnt\", \"landtaxvaluedollarcnt\"]\n",
    "\n",
    "df = prepare.remove_outliers_iqr(df, col_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We noticed there were still a few features with some extreme values so we employed a more conventional method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional outlier removal\n",
    "df = df[((df.bathroomcnt <= 7) & (df.bedroomcnt <= 7) & \n",
    "         (df.bathroomcnt > 0) & \n",
    "         (df.bedroomcnt > 0) & \n",
    "         (df.calculatedfinishedsquarefeet < 7000) & \n",
    "         (df.tax_rate < .05)\n",
    "        )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare.bed_bath_ratio(df)\n",
    "df = prepare.better_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcelDensity30000 = cluster.get_pde(df,30000)\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"The Area We're Focusing On\", fontsize=18)\n",
    "plt.scatter(df['longitude'].values, df['latitude'].values, c=parcelDensity30000,cmap='inferno', s=1, edgecolor='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this section, the team identified patterns in the data which may help to isolate which features were most helpful in predicting churn.\n",
    "\n",
    "- Hypothesis testing in this section assesses the level of correlation between our independent variables and between our independent and target variables\n",
    "- Each descovery about a trend in the data is accompanied by a matching vizualization\n",
    "\n",
    "Before diving into exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature/Feature Hypothesis\n",
    "- $H_0$: Bedroom and bathroom count are not related\n",
    "- $H_a$: Bedroom and bathroom count are related\n",
    "- _alpha ($\\alpha$): 1 - confidence level (95% confidence level -> $\\alpha = .05$)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_scaled.bedroomcnt\n",
    "y = train_scaled.bathroomcnt\n",
    "\n",
    "alternative_hypothesis = 'bedroom count is related to bathroom count'\n",
    "alpha = .05\n",
    "\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "\n",
    "corr, p\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis\")\n",
    "    print(\"We can say that we have confidence that\", alternative_hypothesis)\n",
    "else:\n",
    "    print(\"We fail to reject the null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value/Target Hypothesis\n",
    "- $H_0$: Overall home value has no effect on logerror\n",
    "- $H_a$: Over home value does have an effect on logerror\n",
    "- _alpha ($\\alpha$): 1 - confidence level (95% confidence level -> $\\alpha = .05$)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from scipy import stats\n",
    "\n",
    "x = train_scaled.house_value\n",
    "y = train_scaled.logerror\n",
    "\n",
    "alternative_hypothesis = 'house value is related to logerror'\n",
    "alpha = .05\n",
    "\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "\n",
    "corr, p\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis\")\n",
    "    print(\"We can say that we have confidence that\", alternative_hypothesis)\n",
    "else:\n",
    "    print(\"We fail to reject the null\")\n",
    "    \n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train.logerror,train.house_value)\n",
    "plt.xlabel('Logerror')\n",
    "plt.ylabel('House Value')\n",
    "plt.title('Logerror vs House Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can confrim that although logerror and housevalue are not necesarily linearly correlated, they do seem to have some sort of relationship with eachother. \n",
    "\n",
    "#### We'll look into using house_value as a feature to cluster and perhaps model with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data was split into train and test dataframes (70% and 30% respectively). A random seed was set for reproducibility.\n",
    "- Both sets of data were scaled using the MinMax scaler from SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_scale.train_test(df)\n",
    "print('train:', train.shape)\n",
    "print('test:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler, train_scaled, test_scaled = split_scale.min_max_scaler(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Exploration: Logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to regulate the size of plots for the rest of notebook\n",
    "plt.rc('figure', figsize=(13, 10))\n",
    "plt.rc('font', size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # overall logerror distribution \n",
    "# sns.distplot(df.logerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # absolute value of logerror\n",
    "# sns.kdeplot(df.logerror.abs(), shade=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the correlation between the target variable and all other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Heatmap to display correlation of the features and logerror.\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# sns.heatmap(df.corr(), cmap='Greens', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The heatmap doesn't show any particulary high linear correlation but there could still be certain features that have more of an impact on the target variable\n",
    "\n",
    "#### Let's take a closer look at some specific features :\n",
    "\n",
    "#### Is there a relationship between Bathroom count or Bedroom count and Logerror?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(14,8))\n",
    "# with sns.color_palette('Blues'):\n",
    "#     sns.barplot(x='bathroomcnt', y='logerror', data=train)\n",
    "# plt.xlabel('Bathroom Count')\n",
    "# plt.ylabel('Log Error')\n",
    "# plt.title('Does bathroom count impact log error?')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It looks like homes with smaller bathroom counts tend to produce smaller log error,\n",
    "> ### There are some major errors with home of 6 bathrooms\n",
    "> ### This could possibly be a driver of log error. We'll keep this feature in mind when whe head to clustering and futher modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(14,8))\n",
    "# with sns.color_palette(\"Blues\"):\n",
    "#     sns.barplot(x='bedroomcnt', y='logerror', data=train)\n",
    "# plt.xlabel('Bedroom Count')\n",
    "# plt.ylabel('Log Error')\n",
    "# plt.title('Does bedroom count impact log error?')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedroom count also shows some increse in logerror at higher counts\n",
    "> #### These are the two home features that we can vizually see some variation in log error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(14,8))\n",
    "# sns.scatterplot(x='tax_rate', y='logerror', data=train, alpha=.4)\n",
    "# plt.xlabel('Tax Rate')\n",
    "# plt.ylabel('Log Error')\n",
    "# plt.title('Does tax rate impact log error?')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(14,8))\n",
    "# sns.scatterplot(x='square_footage', y='logerror', data=train, alpha=.4)\n",
    "# plt.xlabel('Finished Square Feet')\n",
    "# plt.ylabel('Log Error')\n",
    "# plt.title('Does the finished square feet of a home impact log error?')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='explore_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features were grouped into 4 categories:\n",
    "> -  Physical position\n",
    "> - Age of home\n",
    "> -  Value of home\n",
    "> - Home features\n",
    "\n",
    "__We constructed several clusters using unique combinations of these features and evaluated them using ttests to determine significance of relationship between each group within the cluster and the target variable *logerror*.__\n",
    "\n",
    "Our two best performing clusters are displayed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Cluster groups according to physical position and home features\n",
    "> - #### Latitude\n",
    "> - #### Longitude\n",
    "> - #### Lot_size\n",
    "> - #### Square_footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_vars = train_scaled[['latitude', 'longitude', 'lot_size', 'square_footage']]\n",
    "cluster_col_name = 'location_size'\n",
    "centroid_col_names = ['centroid_' + i for i in cluster_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine what the best k (number of groups) is\n",
    "# optimal_k = cluster.elbow_method(cluster_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Went with a k of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain:\n",
    "# The train clusters with their observations,\n",
    "# test clusters and their observations\n",
    "# and a df of the number of observations per cluster on train\n",
    "kmeans, train_clusters, test_clusters, cluster_counts = cluster.get_clusters_and_counts(5, ['latitude', 'longitude',\n",
    "                                                                                           'lot_size', 'square_footage'], \n",
    "                                                                                        'location_size', train_scaled, test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = train_scaled[[\"latitude\",\"longitude\",\"square_footage\", \"lot_size\", \"full_value\",\"age\", \"lot_size\"]]\n",
    "X_test_scaled = test_scaled[[\"latitude\",\"longitude\", \"square_footage\", \"lot_size\", \"full_value\",\"age\", \"lot_size\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to obtain:\n",
    "# dataframe of the train clusters with their observations, \n",
    "# test clusters and their observations\n",
    "# and a df of the number of observations per cluster on train. \n",
    "X_train_scaled, train_scaled, X_test_scaled, test_scaled, centroids = cluster.append_clusters_and_centroids(\n",
    "                                X_train_scaled, train_scaled, train_clusters, \n",
    "                                X_test_scaled, test_scaled, test_clusters, \n",
    "                                cluster_col_name, centroid_col_names, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Cluster groups according to one home feature, the age of the home and one value feature\n",
    "> - #### Lot_size\n",
    "> - #### Age\n",
    "> - #### full_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "cluster_vars = train_scaled[['lot_size', 'age', 'full_value']]\n",
    "cluster_col_name = 'size_age_value'\n",
    "centroid_col_names = ['centroid_' + i for i in cluster_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine what the best k (number of groups) is\n",
    "# optimal_k = cluster.elbow_method(cluster_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again we went with a k of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain:\n",
    "# The train clusters with their observations,\n",
    "# test clusters and their observations\n",
    "# and a df of the number of observations per cluster on train\n",
    "kmeans, train_clusters, test_clusters, cluster_counts = cluster.get_clusters_and_counts(5, ['age',\n",
    "                                                                                           'lot_size', 'full_value'], \n",
    "                                                                                        'size_age_value', train_scaled, test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain:\n",
    "# dataframe of the train clusters with their observations, \n",
    "# test clusters and their observations\n",
    "# and a df of the number of observations per cluster on train. \n",
    "X_train_scaled, train_scaled, X_test_scaled, test_scaled, centroids = cluster.append_clusters_and_centroids(\n",
    "                                X_train_scaled, train_scaled, train_clusters, \n",
    "                                X_test_scaled, test_scaled, test_clusters, \n",
    "                                cluster_col_name, centroid_col_names, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.test_sig(X_train_scaled.location_size, train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_vars = train_scaled[['latitude', 'longitude', 'lot_size', 'square_footage']]\n",
    "# cluster_col_name = 'location_size'\n",
    "plt.scatter(train_scaled.square_footage, train_scaled.lot_size, c=X_train_scaled.location_size)\n",
    "plt.xlabel('Square Footage')\n",
    "plt.ylabel('Lot Size')\n",
    "plt.title('Square Footage vs. Lot Size colored by Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.test_sig(X_train_scaled.size_age_value, train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_vars = train_scaled[['lot_size', 'age', 'full_value']]\n",
    "# cluster_col_name = 'size_age_value'\n",
    "plt.scatter(train_scaled.age, train_scaled.full_value, c=X_train_scaled.size_age_value)\n",
    "plt.xlabel('Age of House')\n",
    "plt.ylabel('Full Value of House')\n",
    "plt.title('Age vs. Full Value colored by Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cluster_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_scaled[['lot_size','age','full_value','longitude','latitude','square_footage']]\n",
    "y_train = train_scaled[['logerror']]\n",
    "\n",
    "X_test = test_scaled[['lot_size','age','full_value','longitude','latitude','square_footage']]\n",
    "y_test = test_scaled[['logerror']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = y_train[['logerror']]\n",
    "predictions = predictions.rename(columns={'logerror': 'actual'})\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create baseline\n",
    "X_train = train_scaled[['lot_size','age','full_value','longitude','latitude','square_footage']]\n",
    "y_train = train_scaled[['logerror']]\n",
    "\n",
    "y_train['mean_logerror'] = y_train.logerror.mean()\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_train.logerror, y_train.mean_logerror))\n",
    "r2_baseline = r2_score(y_train.logerror, y_train.mean_logerror)\n",
    "\n",
    "predictions['rsme_baseline'] = ('{:.4f}'.format(rmse_baseline))\n",
    "print('This is the baseline dataset model performance')\n",
    "print(f'RSME = {rmse_baseline:.4f}')\n",
    "print(f'R2 score =  {r2_baseline:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree regressor\n",
    "X_train = train_scaled[['lot_size','age','full_value','longitude','latitude','square_footage']]\n",
    "y_train = train_scaled[['logerror']]\n",
    "\n",
    "regressor = DecisionTreeRegressor(max_depth = 8, random_state=121)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "rmse_dt_train = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r2_dt_train = r2_score(y_train, y_pred)\n",
    "\n",
    "predictions['rsme_dt'] = ('{:.4f}'.format(rmse_dt_train))\n",
    "print('This is the train dataset model performance')\n",
    "print(f'RSME = {rmse_dt_train:.4f}')\n",
    "print(f'R2 score =  {r2_dt_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random forest regressor\n",
    "X_train = train_scaled[['lot_size','age','full_value','longitude','latitude','square_footage']]\n",
    "y_train = train_scaled[['logerror']]\n",
    "\n",
    "regressor = RandomForestRegressor(max_depth = 2, random_state=121, n_estimators=100)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "rmse_rf_train = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r2_rf_train = r2_score(y_train, y_pred)\n",
    "\n",
    "predictions['rsme_rf'] = ('{:.4f}'.format(rmse_rf_train))\n",
    "print('This is the train dataset model performance')\n",
    "print(f'RSME = {rmse_rf_train:.4f}')\n",
    "print(f'R2 score =  {r2_rf_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear model\n",
    "X_train = train_scaled[['lot_size','age','full_value','longitude','latitude','square_footage']]\n",
    "y_train = train_scaled[['logerror']]\n",
    "lm = LinearRegression()\n",
    "# Use Recursive feature selection to find top 3 features\n",
    "rfe = RFE(lm, 3)\n",
    "\n",
    "X_rfe = rfe.fit_transform(X_train,y_train)\n",
    "# Fitting the data to model\n",
    "lm.fit(X_rfe, y_train)\n",
    "\n",
    "y_pred = lm.predict(X_rfe)\n",
    "\n",
    "rmse_lm_train = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r2_lm_train = r2_score(y_train, y_pred)\n",
    "\n",
    "predictions['rsme_lm'] = ('{:.4f}'.format(rmse_lm_train))\n",
    "print('This is the train dataset model performance')\n",
    "print(f'RSME = {rmse_lm_train:.4f}')\n",
    "print(f'R2 score =  {r2_lm_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with Cluster Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a variable out of the clusters\n",
    "\n",
    "X_train_scaled['lot_cluster'] = X_train_scaled['size_age_value'] == 2\n",
    "X_train_scaled['lot_cluster'] = X_train_scaled['lot_cluster'].astype(int)\n",
    "\n",
    "X_train_scaled['loc_cluster'] = X_train_scaled['location_size'] == 3\n",
    "X_train_scaled['loc_cluster'] = X_train_scaled['loc_cluster'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.loc_cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.lot_cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the models against the clusters\n",
    "\n",
    "# create decision tree regressor\n",
    "X_train = X_train_scaled[['loc_cluster']]\n",
    "y_train = train_scaled[['logerror']]\n",
    "\n",
    "regressor = DecisionTreeRegressor(max_depth = 8, random_state=121)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "rmse_dt_train_loc = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r2_dt_train_loc = r2_score(y_train, y_pred)\n",
    "\n",
    "predictions['rsme_dt_loc'] = ('{:.4f}'.format(rmse_dt_train_loc))\n",
    "print('This is the cluster variable dataset using the loc_cluster model performance')\n",
    "print(f'RSME = {rmse_dt_train_loc:.15f}')\n",
    "print(f'R2 score =  {r2_dt_train_loc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree regressor\n",
    "X_train = X_train_scaled[['lot_cluster']]\n",
    "y_train = train_scaled[['logerror']]\n",
    "\n",
    "regressor = DecisionTreeRegressor(max_depth = 8, random_state=121)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "rmse_dt_train_lot = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r2_dt_train_lot = r2_score(y_train, y_pred)\n",
    "\n",
    "predictions['rsme_dt_lot'] = ('{:.4f}'.format(rmse_dt_train_lot))\n",
    "print('This is the cluster variable dataset using the lot_cluster model performance')\n",
    "print(f'RSME = {rmse_dt_train_lot:5f}')\n",
    "print(f'R2 score =  {r2_dt_train_lot:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random forest regressor\n",
    "X_train = X_train_scaled[['loc_cluster']]\n",
    "y_train = train_scaled[['logerror']]\n",
    "\n",
    "regressor = RandomForestRegressor(max_depth = 2, random_state=121, n_estimators=100)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "rmse_rf_train_loc = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r2_rf_train_loc = r2_score(y_train, y_pred)\n",
    "\n",
    "predictions['rsme_rf_loc'] = ('{:.4f}'.format(rmse_rf_train_loc))\n",
    "print('This is the cluster variable dataset model performance')\n",
    "print(f'RSME = {rmse_rf_train_loc:.15f}')\n",
    "print(f'R2 score =  {r2_rf_train_loc:.15f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random forest regressor\n",
    "X_train = X_train_scaled[['lot_cluster']]\n",
    "y_train = train_scaled[['logerror']]\n",
    "\n",
    "regressor = RandomForestRegressor(max_depth = 2, random_state=121, n_estimators=100)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_train)\n",
    "\n",
    "rmse_rf_train_lot = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r2_rf_train_lot = r2_score(y_train, y_pred)\n",
    "\n",
    "predictions['rsme_rf_lot'] = ('{:.4f}'.format(rmse_rf_train_lot))\n",
    "print('This is the cluster variable dataset model performance')\n",
    "print(f'RSME = {rmse_rf_train_lot:.15f}')\n",
    "print(f'R2 score =  {r2_rf_train_lot:.15f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run *best* performing model on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_scaled[['lot_size','age','full_value','longitude','latitude','square_footage']]\n",
    "y_train = train_scaled[['logerror']]\n",
    "X_test = test_scaled[['lot_size','age','full_value','longitude','latitude','square_footage']]\n",
    "y_test = test_scaled[['logerror']]\n",
    "\n",
    "regressor = RandomForestRegressor(max_depth = 2, random_state=121, n_estimators=100)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "rmse_rf_test = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2_rf_test = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "predictions['rsme_rf_test'] = ('{:.4f}'.format(rmse_rf_test))\n",
    "print('This is the test dataset model performance')\n",
    "print(f'RSME = {rmse_rf_test:.15f}')\n",
    "print(f'R2 score =  {r2_rf_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hypothesis and conclusion is unclear. Our derived variables proved useful, but not significantly. \n",
    "\n",
    "2. Our main drivers appeared to hover around the overarching geological data and clustering using the selected features associated with those data points. \n",
    "\n",
    "3. The linear regression model performed quite poorly. However, the decision tree and random forest regressors did slightly better than baseline.\n",
    "\n",
    "4. We observed some statistical difference between log error with regards to these features:\n",
    "    - Longitude/Latitude\n",
    "    - Lot size\n",
    "    - Square footage\n",
    "    - Age of the home\n",
    "    \n",
    "It appears either more time is necessary to evaluate the different clustering opportunities within the data. Or that, perhaps, clustering is not the best approach for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
